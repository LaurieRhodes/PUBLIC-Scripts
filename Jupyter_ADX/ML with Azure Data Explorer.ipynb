{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning with Azure Data Explorer: Room Occupancy Prediction Tutorial\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the complete end-to-end machine learning workflow using Azure Machine Learning (AML) and Azure Data Explorer (ADX) for binary classification of room occupancy. The tutorial covers:\n",
    "\n",
    "- **Data Storage**: Utilizing ADX for efficient time-series data storage and querying\n",
    "- **Model Training**: Training multiple classification models using Azure ML compute clusters\n",
    "- **Model Deployment**: Deploying the trained model for inference using ADX's Python plugin\n",
    "- **Integration**: Seamless integration between ADX and AML for operationalized ML workflows\n",
    "\n",
    "### Business Context\n",
    "\n",
    "Room occupancy prediction is a critical component in:\n",
    "- **Smart Building Management**: Optimizing HVAC systems for energy efficiency\n",
    "- **Space Utilization**: Understanding building usage patterns\n",
    "- **Security Systems**: Detecting unauthorized access or anomalous occupancy patterns\n",
    "- **IoT Analytics**: Processing sensor data at scale\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "The occupancy detection dataset contains environmental sensor readings from an office space:\n",
    "- **Temperature**: Room temperature (°C)\n",
    "- **Humidity**: Relative humidity (%)\n",
    "- **Light**: Illumination levels (Lux)\n",
    "- **CO2**: Carbon dioxide concentration (ppm)\n",
    "- **HumidityRatio**: Derived humidity metric\n",
    "- **Occupancy**: Binary target variable (True/False)\n",
    "- **Test**: Train/test split indicator\n",
    "\n",
    "Ground-truth occupancy labels were obtained from timestamped photographs taken every minute.\n",
    "\n",
    "*Original example adapted from Mohammad Ghodratigohar's [YouTube tutorial](https://www.youtube.com/watch?v=wtCQ7vI9_60)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites and Setup\n",
    "\n",
    "### Azure Data Explorer Requirements\n",
    "\n",
    "Before proceeding, ensure your ADX cluster is properly configured:\n",
    "\n",
    "1. **Enable Python Plugin**: The Python plugin must be enabled on your ADX cluster. This is required for model inference.\n",
    "   - Navigate to your ADX cluster in the Azure portal\n",
    "   - Go to Configuration → Python plugin\n",
    "   - Enable the plugin (currently tested with Python Python 3.11.7 DL image)\n",
    "   - Documentation: [Python Plugin Guide](https://docs.microsoft.com/en-us/azure/data-explorer/kusto/query/pythonplugin?pivots=azuredataexplorer)\n",
    "\n",
    "2. **Create Sample Dataset**: Execute the following KQL commands in your ADX cluster:\n",
    "\n",
    "```kql\n",
    "// Create the OccupancyDetection table with proper schema\n",
    ".create table OccupancyDetection (\n",
    "    Timestamp: datetime,\n",
    "    Temperature: real,\n",
    "    Humidity: real,\n",
    "    Light: real,\n",
    "    CO2: real,\n",
    "    HumidityRatio: real,\n",
    "    Occupancy: bool,\n",
    "    Test: bool\n",
    ")\n",
    "\n",
    "// Clear existing data and insert sample data with balanced train/test split\n",
    ".clear table OccupancyDetection data\n",
    "\n",
    "// Insert sample data with clear occupied vs unoccupied patterns\n",
    ".ingest inline into table OccupancyDetection <|\n",
    "2015-02-18T09:16:00.0000000Z,22.5,32.0,450.0,1800.0,0.0055,true,false\n",
    "2015-02-18T09:17:00.0000000Z,23.0,33.5,480.0,1950.0,0.0058,true,false\n",
    "2015-02-18T09:18:00.0000000Z,22.8,32.8,465.0,1875.0,0.0057,true,false\n",
    "2015-02-18T09:19:00.0000000Z,23.2,34.0,490.0,2000.0,0.0059,true,false\n",
    "2015-02-18T09:20:00.0000000Z,22.9,33.2,470.0,1920.0,0.0058,true,false\n",
    "2015-02-18T10:16:00.0000000Z,18.5,22.0,0.0,350.0,0.0032,false,false\n",
    "2015-02-18T10:17:00.0000000Z,18.2,21.5,0.0,340.0,0.0031,false,false\n",
    "2015-02-18T10:18:00.0000000Z,18.0,21.0,0.0,330.0,0.0030,false,false\n",
    "2015-02-18T10:19:00.0000000Z,18.3,21.8,0.0,345.0,0.0032,false,false\n",
    "2015-02-18T10:20:00.0000000Z,18.1,21.3,0.0,335.0,0.0031,false,false\n",
    "2015-02-18T11:16:00.0000000Z,22.7,32.5,455.0,1850.0,0.0056,true,true\n",
    "2015-02-18T11:17:00.0000000Z,23.1,33.8,475.0,1900.0,0.0058,true,true\n",
    "2015-02-18T11:18:00.0000000Z,22.6,32.2,460.0,1820.0,0.0055,true,true\n",
    "2015-02-18T12:16:00.0000000Z,18.4,22.2,0.0,355.0,0.0033,false,true\n",
    "2015-02-18T12:17:00.0000000Z,18.0,21.2,0.0,325.0,0.0030,false,true\n",
    "2015-02-18T12:18:00.0000000Z,18.6,22.5,0.0,360.0,0.0034,false,true\n",
    "```\n",
    "\n",
    "### Azure Machine Learning Requirements\n",
    "\n",
    "- An Azure ML workspace\n",
    "- A compute cluster for training (will be created if not exists)\n",
    "- Appropriate Azure Storage account with blob container\n",
    "- Required Python packages (installed below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup and Package Installation\n",
    "\n",
    "Installing required packages with version pinning for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751320754126
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment, ScriptRunConfig\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Install KqlMagic with quiet output to avoid verbose logs\n",
    "!pip install Kqlmagic --no-cache-dir --upgrade -q\n",
    "\n",
    "# Display version information for troubleshooting\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Azure ML SDK Version: {azureml.core.VERSION}\")\n",
    "print(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Workspace Configuration\n",
    "\n",
    "### Workspace Creation\n",
    "\n",
    "Choose one of the following approaches based on your scenario:\n",
    "\n",
    "1. **Create New Workspace**: Use the first cell if you need to create a new workspace\n",
    "2. **Use Existing Workspace**: Use the second cell if connecting to an existing workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751095633017
    }
   },
   "outputs": [],
   "source": [
    "# Option 1: Create a new Azure ML workspace\n",
    "# Update these values with your Azure subscription details\n",
    "SUBSCRIPTION_ID = \"your-subscription-id\"\n",
    "RESOURCE_GROUP = \"your-resource-group\"\n",
    "WORKSPACE_NAME = \"your-workspace-name\"\n",
    "LOCATION = \"your-azure-region\"  # e.g., 'eastus', 'westeurope'\n",
    "\n",
    "# Uncomment the following lines to create a new workspace\n",
    "# ws = Workspace.create(\n",
    "#     name=WORKSPACE_NAME,\n",
    "#     subscription_id=SUBSCRIPTION_ID,\n",
    "#     resource_group=RESOURCE_GROUP,\n",
    "#     location=LOCATION,\n",
    "#     exist_ok=True,\n",
    "#     show_output=True\n",
    "# )\n",
    "# \n",
    "# # Save workspace configuration for future use\n",
    "# ws.write_config()\n",
    "\n",
    "print(\"Workspace creation cell - modify and uncomment as needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751096947507
    },
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# Option 2: Connect to existing workspace using config.json\n",
    "# This assumes you have a config.json file in the current directory\n",
    "# or you've previously run ws.write_config()\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(f\"Connected to workspace: {ws.name}\")\n",
    "    print(f\"Location: {ws.location}\")\n",
    "    print(f\"Resource Group: {ws.resource_group}\")\n",
    "    print(f\"Subscription ID: {ws.subscription_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to workspace: {e}\")\n",
    "    print(\"Please ensure config.json exists or create workspace using Option 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment and Environment Setup\n",
    "\n",
    "### Create Experiment\n",
    "\n",
    "Experiments in Azure ML provide organization and tracking for related runs. All training jobs will be tracked under this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751096957642
    },
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "# Create experiment for tracking all occupancy prediction runs\n",
    "EXPERIMENT_NAME = \"occupancy-prediction-adx\"\n",
    "exp = Experiment(workspace=ws, name=EXPERIMENT_NAME)\n",
    "\n",
    "# Create custom environment with modern, stable package versions\n",
    "# This replaces the legacy approach of hardcoded package versions\n",
    "env = Environment(name=\"occupancy-detection-env-modern\")\n",
    "\n",
    "# Define conda dependencies with current best practices\n",
    "conda_deps = CondaDependencies()\n",
    "\n",
    "# Use Python 3.9 for broad compatibility and stability\n",
    "conda_deps.add_conda_package(\"python=3.9\")\n",
    "\n",
    "# Core data science packages with minimum versions for compatibility\n",
    "conda_deps.add_conda_package(\"pandas>=1.3.0\")\n",
    "conda_deps.add_conda_package(\"scikit-learn>=1.0.0\")\n",
    "conda_deps.add_conda_package(\"numpy>=1.21.0\")\n",
    "\n",
    "# Azure ML specific packages\n",
    "conda_deps.add_pip_package(\"azureml-defaults\")\n",
    "conda_deps.add_pip_package(\"azure-storage-blob>=12.0.0\")\n",
    "\n",
    "# Assign dependencies to environment\n",
    "env.python.conda_dependencies = conda_deps\n",
    "\n",
    "print(f\"Experiment '{EXPERIMENT_NAME}' created successfully\")\n",
    "print(\"Custom environment configured with modern package versions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Infrastructure Setup\n",
    "\n",
    "### Create or Connect to Compute Cluster\n",
    "\n",
    "Azure ML Compute provides managed, scalable compute for training. The cluster will:\n",
    "\n",
    "- **Auto-scale**: Start with minimum nodes and scale up as needed\n",
    "- **Cost-efficient**: Scale down to minimum nodes when idle\n",
    "- **Managed**: Azure handles OS updates, driver installation, etc.\n",
    "\n",
    "**Note**: Initial cluster creation takes approximately 5 minutes. Subsequent runs reuse the existing cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751096993976
    },
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute cluster configuration\n",
    "COMPUTE_NAME = \"cpu-cluster\"\n",
    "VM_SIZE = \"STANDARD_D2_V2\"  # 2 cores, 7GB RAM - suitable for small ML workloads\n",
    "MIN_NODES = 0  # Cost optimization: scale to zero when idle\n",
    "MAX_NODES = 2  # Limit maximum nodes to control costs\n",
    "\n",
    "# Check if compute target already exists\n",
    "if COMPUTE_NAME in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[COMPUTE_NAME]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(f\"Found existing compute target: {COMPUTE_NAME}\")\n",
    "        print(f\"VM Size: {compute_target.vm_size}\")\n",
    "        print(f\"Current Nodes: {compute_target.get_status().current_node_count}\")\n",
    "    else:\n",
    "        print(f\"Compute target {COMPUTE_NAME} exists but is not AmlCompute type\")\n",
    "else:\n",
    "    print(f\"Creating new compute target: {COMPUTE_NAME}\")\n",
    "    print(f\"VM Size: {VM_SIZE}, Min Nodes: {MIN_NODES}, Max Nodes: {MAX_NODES}\")\n",
    "    \n",
    "    # Create compute cluster configuration\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=VM_SIZE,\n",
    "        min_nodes=MIN_NODES,\n",
    "        max_nodes=MAX_NODES,\n",
    "        idle_seconds_before_scaledown=300  # Scale down after 5 minutes of inactivity\n",
    "    )\n",
    "    \n",
    "    # Create the compute target\n",
    "    compute_target = ComputeTarget.create(ws, COMPUTE_NAME, provisioning_config)\n",
    "    \n",
    "    # Wait for completion with timeout\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True,\n",
    "        min_node_count=None,\n",
    "        timeout_in_minutes=10\n",
    "    )\n",
    "    \n",
    "    print(\"Compute cluster created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration with Azure Data Explorer\n",
    "\n",
    "### KqlMagic Configuration\n",
    "\n",
    "KqlMagic enables seamless integration between Jupyter notebooks and Azure Data Explorer. The configuration below addresses common connection issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751096998229
    }
   },
   "outputs": [],
   "source": [
    "# Configure KqlMagic to prevent common connection errors\n",
    "# These settings optimize the interaction with ADX\n",
    "%config Kqlmagic.auto_dataframe = True\n",
    "%config Kqlmagic.short_errors = True\n",
    "%config Kqlmagic.display_limit = 50\n",
    "\n",
    "# Set environment variable for Azure ML compatibility\n",
    "import os\n",
    "os.environ['KQLMAGIC_AZUREML_COMPUTE'] = 'ml.azure.com'\n",
    "\n",
    "# Reload the extension to apply configuration\n",
    "%reload_ext Kqlmagic\n",
    "\n",
    "print(\"KqlMagic configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure Data Explorer\n",
    "\n",
    "**Important**: Update the connection string below with your ADX cluster details:\n",
    "\n",
    "- **cluster**: Your ADX cluster URI (e.g., 'yourcluster.region.kusto.windows.net')\n",
    "- **database**: Your database name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097036767
    }
   },
   "outputs": [],
   "source": [
    "# Update these values with your ADX cluster details\n",
    "ADX_CLUSTER = \"your-adx-cluster.australiasoutheast.kusto.windows.net\"\n",
    "ADX_DATABASE = \"your-database\"\n",
    "\n",
    "# Connect to Azure Data Explorer\n",
    "# This will prompt for device authentication\n",
    "%kql azureDataExplorer://code;cluster='{ADX_CLUSTER}';database='{ADX_DATABASE}'\n",
    "\n",
    "print(f\"Connected to ADX cluster: {ADX_CLUSTER}\")\n",
    "print(f\"Database: {ADX_DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097042477
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query the occupancy detection dataset\n",
    "# Store results in 'df' variable for further analysis\n",
    "%kql df << OccupancyDetection\n",
    "\n",
    "# Display dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Show first few records\n",
    "print(\"\\nFirst 5 records:\")\n",
    "display(df.head())\n",
    "\n",
    "# Basic data quality checks\n",
    "print(\"\\nData quality summary:\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "print(f\"Training records (Test=False): {len(df[df['Test'] == False])}\")\n",
    "print(f\"Testing records (Test=True): {len(df[df['Test'] == True])}\")\n",
    "print(f\"Occupied records: {len(df[df['Occupancy'] == True])}\")\n",
    "print(f\"Unoccupied records: {len(df[df['Occupancy'] == False])}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"\\nMissing values detected:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nNo missing values detected.\")\n",
    "\n",
    "print(\"\\nData exploration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Export to Azure Storage\n",
    "\n",
    "### Storage Configuration\n",
    "\n",
    "To train models in Azure ML, we need to export data from ADX to Azure Blob Storage. This approach provides:\n",
    "\n",
    "- **Scalability**: Handle large datasets efficiently\n",
    "- **Integration**: Seamless connection between ADX and AML\n",
    "- **Versioning**: Track different versions of training data\n",
    "\n",
    "**Storage Setup Instructions:**\n",
    "\n",
    "1. Use the storage account automatically created with your AML workspace, or create a new one\n",
    "2. Create a blob container (e.g., 'ml-data')\n",
    "3. Generate a SAS token with read/write permissions\n",
    "4. You can use [Azure Storage Explorer](https://azure.microsoft.com/features/storage-explorer/) for easy management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097052478
    }
   },
   "outputs": [],
   "source": [
    "# Azure Storage configuration\n",
    "# Update these values with your storage account details\n",
    "STORAGE_ACCOUNT = \"your-storage-account-name\"\n",
    "CONTAINER_NAME = \"your-container-name\n",
    "SAS_TOKEN = \"?sp=sastoken..\"  # Your SAS token starting with ?\n",
    "\n",
    "# Validate configuration\n",
    "if STORAGE_ACCOUNT == \"your-storage-account-name\":\n",
    "    print(\"⚠️  Please update the storage configuration with your actual values\")\n",
    "    print(\"   - STORAGE_ACCOUNT: Your Azure Storage account name\")\n",
    "    print(\"   - CONTAINER_NAME: Your blob container name\")\n",
    "    print(\"   - SAS_TOKEN: Your container SAS token with read/write permissions\")\n",
    "else:\n",
    "    print(f\"✅ Storage configured: {STORAGE_ACCOUNT}/{CONTAINER_NAME}\")\n",
    "    \n",
    "    # Construct blob URI for ADX export\n",
    "    blob_container_uri = f\"https://{STORAGE_ACCOUNT}.blob.core.windows.net/{CONTAINER_NAME}{SAS_TOKEN}\"\n",
    "    print(f\"Blob URI configured for data export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097055808
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Export data from ADX to Azure Blob Storage\n",
    "# This creates a CSV file that Azure ML can access\n",
    "\n",
    "if 'blob_container_uri' not in locals():\n",
    "    print(\"❌ Please configure storage settings in the previous cell first\")\n",
    "else:\n",
    "    # Construct the ADX export command\n",
    "    export_query = f\".export to csv (h@'{blob_container_uri}') with(includeHeaders=all) <| OccupancyDetection\"\n",
    "    \n",
    "    print(\"Exporting data from ADX to blob storage...\")\n",
    "    print(f\"Export command: {export_query[:100]}...\")\n",
    "    \n",
    "    # Execute the export\n",
    "    %kql export_result << -query export_query\n",
    "    \n",
    "    # Validate export results\n",
    "    if not export_result.empty and 'Path' in export_result.columns:\n",
    "        # Extract the blob file name from the full path\n",
    "        blob_path = export_result['Path'].iloc[0]\n",
    "        data_blob_name = blob_path.split('/')[-1]\n",
    "        \n",
    "        print(f\"✅ Export successful!\")\n",
    "        print(f\"   Records exported: {export_result['NumRecords'].iloc[0]}\")\n",
    "        print(f\"   File size: {export_result['SizeInBytes'].iloc[0]} bytes\")\n",
    "        print(f\"   Blob name: {data_blob_name}\")\n",
    "        \n",
    "        # Store for use in training script\n",
    "        EXPORTED_BLOB_NAME = data_blob_name\n",
    "    else:\n",
    "        print(\"❌ Export failed - check table data and storage permissions\")\n",
    "        print(\"   Ensure the OccupancyDetection table contains data\")\n",
    "        print(\"   Verify SAS token has write permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation\n",
    "\n",
    "Verify the exported data by downloading and inspecting it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097059872
    }
   },
   "outputs": [],
   "source": [
    "# Validate exported data by downloading and inspecting\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "if 'EXPORTED_BLOB_NAME' not in locals():\n",
    "    print(\"❌ No exported blob found. Please run the export cell above first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Create blob service client\n",
    "        blob_service_client = BlobServiceClient(\n",
    "            account_url=f\"https://{STORAGE_ACCOUNT}.blob.core.windows.net\",\n",
    "            credential=SAS_TOKEN.lstrip('?')  # Remove the ? prefix\n",
    "        )\n",
    "        \n",
    "        # Download blob to local file\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=CONTAINER_NAME,\n",
    "            blob=EXPORTED_BLOB_NAME\n",
    "        )\n",
    "        \n",
    "        # Download and read the CSV\n",
    "        with open('validation_data.csv', 'wb') as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "        \n",
    "        # Load and validate the data\n",
    "        validation_df = pd.read_csv('validation_data.csv')\n",
    "        \n",
    "        print(\"✅ Data validation successful!\")\n",
    "        print(f\"   Downloaded file shape: {validation_df.shape}\")\n",
    "        print(f\"   Columns: {validation_df.columns.tolist()}\")\n",
    "        \n",
    "        # Display sample of exported data\n",
    "        print(\"\\nSample of exported data:\")\n",
    "        display(validation_df.tail(5))\n",
    "        \n",
    "        print(\"Data export validation complete.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation failed: {e}\")\n",
    "        print(\"   Check storage account configuration and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Setup\n",
    "\n",
    "### Create Training Script Directory\n",
    "\n",
    "Azure ML requires all training files to be in a directory that will be uploaded to the compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097065283
    }
   },
   "outputs": [],
   "source": [
    "# Create directory for training script and related files\n",
    "import os\n",
    "\n",
    "SCRIPT_FOLDER = os.path.join(os.getcwd(), \"training_scripts\")\n",
    "os.makedirs(SCRIPT_FOLDER, exist_ok=True)\n",
    "\n",
    "print(f\"Training script directory created: {SCRIPT_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Script Development\n",
    "\n",
    "The training script below implements:\n",
    "\n",
    "- **Data Loading**: Downloads data from Azure Blob Storage\n",
    "- **Data Preprocessing**: Handles train/test splits and data validation\n",
    "- **Model Training**: Trains multiple classification algorithms\n",
    "- **Model Evaluation**: Cross-validation with appropriate fold sizing\n",
    "- **Model Persistence**: Saves trained models for deployment\n",
    "- **Experiment Tracking**: Logs metrics to Azure ML\n",
    "\n",
    "**Algorithms Evaluated:**\n",
    "- Decision Tree Classifier\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors\n",
    "- Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1750829234782
    }
   },
   "outputs": [],
   "source": [
    "%%writefile \"$SCRIPT_FOLDER/train.py\"\n",
    "\n",
    "\"\"\"\n",
    "Azure ML Training Script for Room Occupancy Prediction\n",
    "\n",
    "This script trains multiple classification models on environmental sensor data\n",
    "to predict room occupancy. It includes comprehensive error handling, \n",
    "data validation, and experiment tracking.\n",
    "\n",
    "Authors: Adapted from Mohammad Ghodratigohar's original example\n",
    "Date: June 2025\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn import tree\n",
    "from sklearn import neighbors\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Azure ML imports\n",
    "from azureml.core import Run\n",
    "import os\n",
    "\n",
    "def parse_arguments():\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Train occupancy prediction models')\n",
    "    parser.add_argument('--account', type=str, required=True, \n",
    "                       help='Azure Storage account name')\n",
    "    parser.add_argument('--container', type=str, required=True,\n",
    "                       help='Blob container name')\n",
    "    parser.add_argument('--blob', type=str, required=True,\n",
    "                       help='Blob file name')\n",
    "    parser.add_argument('--sas', type=str, required=True,\n",
    "                       help='SAS token for blob access')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def download_data(storage_account, container_name, blob_name, sas_token):\n",
    "    \"\"\"Download training data from Azure Blob Storage.\"\"\"\n",
    "    try:\n",
    "        print(f\"Downloading data from blob: {blob_name}\")\n",
    "        \n",
    "        # Create blob service client\n",
    "        blob_service_client = BlobServiceClient(\n",
    "            account_url=f\"https://{storage_account}.blob.core.windows.net\",\n",
    "            credential=sas_token.lstrip('?')\n",
    "        )\n",
    "        \n",
    "        # Download blob\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name,\n",
    "            blob=blob_name\n",
    "        )\n",
    "        \n",
    "        with open('training_data.csv', 'wb') as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "        \n",
    "        print(\"✅ Data download successful\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_and_validate_data():\n",
    "    \"\"\"Load CSV data and perform validation checks.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('training_data.csv')\n",
    "        \n",
    "        print(f\"Dataset loaded: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Validate required columns\n",
    "        required_columns = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio', 'Occupancy', 'Test']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        if df.isnull().sum().sum() > 0:\n",
    "            print(\"⚠️  Missing values detected:\")\n",
    "            print(df.isnull().sum())\n",
    "        \n",
    "        # Display data distribution\n",
    "        print(f\"\\nData distribution:\")\n",
    "        print(f\"  Total records: {len(df)}\")\n",
    "        print(f\"  Occupied: {len(df[df['Occupancy'] == True])}\")\n",
    "        print(f\"  Unoccupied: {len(df[df['Occupancy'] == False])}\")\n",
    "        print(f\"  Training records: {len(df[df['Test'] == False])}\")\n",
    "        print(f\"  Testing records: {len(df[df['Test'] == True])}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Data validation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_features_and_targets(df):\n",
    "    \"\"\"Prepare feature matrices and target vectors for training.\"\"\"\n",
    "    feature_columns = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']\n",
    "    \n",
    "    # Check if we have proper train/test split\n",
    "    train_available = (df['Test'] == False).any()\n",
    "    test_available = (df['Test'] == True).any()\n",
    "    \n",
    "    if not train_available or not test_available:\n",
    "        print(\"⚠️  No proper train/test split found. Creating manual split (70/30)...\")\n",
    "        n_train = int(0.7 * len(df))\n",
    "        train_data = df.iloc[:n_train].copy()\n",
    "        test_data = df.iloc[n_train:].copy()\n",
    "    else:\n",
    "        train_data = df[df['Test'] == False].copy()\n",
    "        test_data = df[df['Test'] == True].copy()\n",
    "    \n",
    "    # Prepare training data\n",
    "    train_x = train_data[feature_columns]\n",
    "    train_y = train_data['Occupancy']\n",
    "    \n",
    "    # Prepare testing data\n",
    "    test_x = test_data[feature_columns]\n",
    "    test_y = test_data['Occupancy']\n",
    "    \n",
    "    print(f\"\\nData split summary:\")\n",
    "    print(f\"  Training samples: {len(train_x)}\")\n",
    "    print(f\"  Testing samples: {len(test_x)}\")\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "def create_models():\n",
    "    \"\"\"Create and configure classification models.\"\"\"\n",
    "    models = {\n",
    "        'Decision Tree': tree.DecisionTreeClassifier(random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(\n",
    "            solver='liblinear', max_iter=1000, random_state=42\n",
    "        ),\n",
    "        'K Nearest Neighbors': neighbors.KNeighborsClassifier(n_neighbors=3),\n",
    "        'Naive Bayes': naive_bayes.GaussianNB()\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def train_and_evaluate_models(models, train_x, train_y, test_x, test_y, run):\n",
    "    \"\"\"Train models and evaluate performance.\"\"\"\n",
    "    \n",
    "    # Create outputs directory for model persistence\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    \n",
    "    # Calculate appropriate CV folds based on dataset size\n",
    "    min_samples_per_class = min(train_y.sum(), len(train_y) - train_y.sum())\n",
    "    train_cv_folds = min(5, max(2, min_samples_per_class))\n",
    "    \n",
    "    test_min_samples = min(test_y.sum(), len(test_y) - test_y.sum())\n",
    "    test_cv_folds = min(3, max(2, test_min_samples))\n",
    "    \n",
    "    print(f\"\\nUsing {train_cv_folds} folds for training CV, {test_cv_folds} folds for testing CV\")\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}...\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            # Train the model\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            # Training accuracy with cross-validation\n",
    "            if len(train_x) >= train_cv_folds:\n",
    "                train_scores = cross_val_score(\n",
    "                    model, train_x, train_y, \n",
    "                    cv=train_cv_folds, scoring='accuracy'\n",
    "                )\n",
    "                train_accuracy = train_scores.mean()\n",
    "                train_std = train_scores.std()\n",
    "            else:\n",
    "                train_accuracy = model.score(train_x, train_y)\n",
    "                train_std = 0.0\n",
    "            \n",
    "            # Testing accuracy with cross-validation\n",
    "            if len(test_x) >= test_cv_folds:\n",
    "                test_scores = cross_val_score(\n",
    "                    model, test_x, test_y,\n",
    "                    cv=test_cv_folds, scoring='accuracy'\n",
    "                )\n",
    "                # Handle NaN values from failed CV folds (e.g., KNN with small datasets)\n",
    "                valid_scores = test_scores[~np.isnan(test_scores)]\n",
    "                if len(valid_scores) > 0:\n",
    "                    test_accuracy = valid_scores.mean()\n",
    "                    test_std = valid_scores.std()\n",
    "                else:\n",
    "                    test_accuracy = model.score(test_x, test_y)\n",
    "                    test_std = 0.0\n",
    "            else:\n",
    "                test_accuracy = model.score(test_x, test_y)\n",
    "                test_std = 0.0\n",
    "            \n",
    "            # Log metrics to Azure ML\n",
    "            run.log(f'{model_name}_training_accuracy', train_accuracy)\n",
    "            run.log(f'{model_name}_testing_accuracy', test_accuracy)\n",
    "            run.log('model_type', model_name)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"Training Accuracy: {train_accuracy:.4f} (+/- {train_std:.4f})\")\n",
    "            print(f\"Testing Accuracy:  {test_accuracy:.4f} (+/- {test_std:.4f})\")\n",
    "            \n",
    "            # Generate detailed classification report for test set\n",
    "            test_predictions = model.predict(test_x)\n",
    "            print(f\"\\nClassification Report:\")\n",
    "            print(classification_report(test_y, test_predictions))\n",
    "            \n",
    "            # Save model\n",
    "            model_filename = f'outputs/{model_name.replace(\" \", \"_\")}.pkl'\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump(model, f)\n",
    "            \n",
    "            print(f\"Model saved: {model_filename}\")\n",
    "            \n",
    "            # Store results\n",
    "            model_results[model_name] = {\n",
    "                'training_accuracy': train_accuracy,\n",
    "                'testing_accuracy': test_accuracy,\n",
    "                'model': model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error training {model_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return model_results\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    print(\"🚀 Starting occupancy prediction model training...\")\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    args = parse_arguments()\n",
    "    \n",
    "    # Get Azure ML run context\n",
    "    run = Run.get_context()\n",
    "    \n",
    "    # Download data\n",
    "    if not download_data(args.account, args.container, args.blob, args.sas):\n",
    "        return\n",
    "    \n",
    "    # Load and validate data\n",
    "    df = load_and_validate_data()\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    # Prepare features and targets\n",
    "    train_x, train_y, test_x, test_y = prepare_features_and_targets(df)\n",
    "    \n",
    "    # Create models\n",
    "    models = create_models()\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results = train_and_evaluate_models(models, train_x, train_y, test_x, test_y, run)\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if results:\n",
    "        best_model = max(results.items(), key=lambda x: x[1]['testing_accuracy'])\n",
    "        \n",
    "        print(\"Model Performance Comparison:\")\n",
    "        for name, metrics in results.items():\n",
    "            marker = \"🏆\" if name == best_model[0] else \"📊\"\n",
    "            print(f\"  {marker} {name}: {metrics['testing_accuracy']:.4f} (test accuracy)\")\n",
    "        \n",
    "        print(f\"\\n🎯 Best performing model: {best_model[0]} (Test Accuracy: {best_model[1]['testing_accuracy']:.4f})\")\n",
    "        \n",
    "        # Log best model info\n",
    "        run.log('best_model', best_model[0])\n",
    "        run.log('best_test_accuracy', best_model[1]['testing_accuracy'])\n",
    "    \n",
    "    print(\"✅ Training completed successfully!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Execution\n",
    "\n",
    "### Modern Training Approach with ScriptRunConfig\n",
    "\n",
    "We use Azure ML's modern `ScriptRunConfig` approach instead of the deprecated estimator pattern. This provides:\n",
    "\n",
    "- **Flexibility**: Full control over the training environment\n",
    "- **Consistency**: Unified approach for all framework types\n",
    "- **Maintainability**: Future-proof against SDK changes\n",
    "- **Transparency**: Clear separation of environment and execution logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097083718
    }
   },
   "outputs": [],
   "source": [
    "# Validate that all required variables are available\n",
    "EXPORTED_BLOB_NAME = data_blob_name\n",
    "\n",
    "print(\"✅ Variables mapped successfully:\")\n",
    "print(f\"   Storage Account: {STORAGE_ACCOUNT}\")\n",
    "print(f\"   Container: {CONTAINER_NAME}\")\n",
    "print(f\"   Blob: {EXPORTED_BLOB_NAME}\")\n",
    "\n",
    "# Create ScriptRunConfig for modern, flexible training job submission\n",
    "script_config = ScriptRunConfig(\n",
    "    source_directory=SCRIPT_FOLDER,\n",
    "    script='train.py',\n",
    "    environment=env,\n",
    "    compute_target=compute_target,\n",
    "    arguments=[\n",
    "        '--account', STORAGE_ACCOUNT,\n",
    "        '--container', CONTAINER_NAME,\n",
    "        '--blob', EXPORTED_BLOB_NAME,\n",
    "        '--sas', SAS_TOKEN\n",
    "    ]\n",
    ")\n",
    "    \n",
    "print(\"Training configuration created successfully\")\n",
    "print(f\"Script directory: {SCRIPT_FOLDER}\")\n",
    "print(f\"Compute target: {compute_target.name}\")\n",
    "print(f\"Environment: {env.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Training Job\n",
    "\n",
    "Submit the training job to the compute cluster. The job will:\n",
    "\n",
    "1. **Environment Setup** (~2-5 minutes): Create or reuse Docker environment\n",
    "2. **Data Download**: Fetch training data from blob storage\n",
    "3. **Model Training**: Train all four classification models\n",
    "4. **Evaluation**: Perform cross-validation and generate metrics\n",
    "5. **Model Persistence**: Save trained models to outputs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097093967
    },
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [],
   "source": [
    "# Submit the training job\n",
    "if 'script_config' in locals():\n",
    "    print(\"🚀 Submitting training job to Azure ML...\")\n",
    "    \n",
    "    # Submit the run\n",
    "    training_run = exp.submit(config=script_config)\n",
    "    \n",
    "    print(f\"✅ Training job submitted successfully!\")\n",
    "    print(f\"Run ID: {training_run.id}\")\n",
    "    print(f\"Experiment: {exp.name}\")\n",
    "    print(f\"Status: {training_run.get_status()}\")\n",
    "    \n",
    "    # Display Azure ML Studio link\n",
    "    studio_url = training_run.get_portal_url()\n",
    "    print(f\"\\n🔗 Monitor progress in Azure ML Studio:\")\n",
    "    print(f\"   {studio_url}\")\n",
    "    \n",
    "    # Store run reference for monitoring\n",
    "    current_run = training_run\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot submit job - training configuration not available\")\n",
    "    print(\"   Please run the configuration cell above first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Training Progress\n",
    "\n",
    "You can monitor the training job in multiple ways:\n",
    "\n",
    "1. **Azure ML Studio**: Use the link above for a rich web interface\n",
    "2. **Notebook Widget**: Execute the cell below for inline monitoring\n",
    "3. **Programmatic Status**: Check status and logs programmatically\n",
    "\n",
    "**Training Timeline:**\n",
    "- **Image Creation** (first run only): ~5 minutes\n",
    "- **Cluster Scaling**: ~2-3 minutes if nodes need to be allocated\n",
    "- **Job Execution**: ~2-5 minutes for this dataset size\n",
    "- **Post-processing**: ~1 minute to collect outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097151211
    },
    "tags": [
     "use notebook widget"
    ]
   },
   "outputs": [],
   "source": [
    "# Monitor training job with real-time output\n",
    "if 'current_run' in locals():\n",
    "    print(f\"Monitoring training run: {current_run.id}\")\n",
    "    print(f\"Current status: {current_run.get_status()}\")\n",
    "    \n",
    "    # Wait for completion with live output streaming\n",
    "    # Set show_output=True to see real-time logs from the training script\n",
    "    try:\n",
    "        result = current_run.wait_for_completion(show_output=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"TRAINING COMPLETED\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"Final Status: {result['status']}\")\n",
    "        print(f\"Start Time: {result['startTimeUtc']}\")\n",
    "        print(f\"End Time: {result['endTimeUtc']}\")\n",
    "        \n",
    "        # Display key metrics\n",
    "        metrics = current_run.get_metrics()\n",
    "        if metrics:\n",
    "            print(\"\\n📊 Training Metrics:\")\n",
    "            for key, value in metrics.items():\n",
    "                if 'accuracy' in key.lower():\n",
    "                    print(f\"  {key}: {value:.4f}\")\n",
    "        \n",
    "        # List output files\n",
    "        files = current_run.get_file_names()\n",
    "        output_files = [f for f in files if f.startswith('outputs/')]\n",
    "        if output_files:\n",
    "            print(f\"\\n💾 Generated Model Files:\")\n",
    "            for file in output_files:\n",
    "                print(f\"  {file}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n⏸️  Monitoring interrupted by user\")\n",
    "        print(f\"   Training continues in background. Check status with: current_run.get_status()\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error during monitoring: {e}\")\n",
    "        print(f\"   Check run status manually: {current_run.get_status()}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No active training run found\")\n",
    "    print(\"   Please submit a training job first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Training Results\n",
    "\n",
    "After training completes, we can analyze the results and download the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097163673
    }
   },
   "outputs": [],
   "source": [
    "# Analyze training results and download best model\n",
    "if 'current_run' in locals() and current_run.get_status() == 'Completed':\n",
    "    \n",
    "    print(\"📈 Analyzing training results...\")\n",
    "    \n",
    "    # Get all metrics\n",
    "    metrics = current_run.get_metrics()\n",
    "    \n",
    "    # Extract model performance metrics\n",
    "    model_accuracies = {}\n",
    "    for key, value in metrics.items():\n",
    "        if '_testing_accuracy' in key:\n",
    "            model_name = key.replace('_testing_accuracy', '')\n",
    "            model_accuracies[model_name] = value\n",
    "    \n",
    "    if model_accuracies:\n",
    "        print(\"\\n🏆 Model Performance Summary:\")\n",
    "        sorted_models = sorted(model_accuracies.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (model, accuracy) in enumerate(sorted_models):\n",
    "            rank_emoji = \"🥇\" if i == 0 else \"🥈\" if i == 1 else \"🥉\" if i == 2 else \"📊\"\n",
    "            print(f\"  {rank_emoji} {model}: {accuracy:.4f}\")\n",
    "        \n",
    "        # Download best model\n",
    "        best_model_name, best_accuracy = sorted_models[0]\n",
    "        model_filename = f\"{best_model_name.replace(' ', '_')}.pkl\"\n",
    "        \n",
    "        print(f\"\\n💾 Downloading best model: {best_model_name}\")\n",
    "        try:\n",
    "            # Download the best model file\n",
    "            current_run.download_file(f\"outputs/{model_filename}\", output_file_path=model_filename)\n",
    "            print(f\"   ✅ Model downloaded: {model_filename}\")\n",
    "            print(f\"   🎯 Best accuracy: {best_accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Download failed: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️  No model accuracy metrics found\")\n",
    "    \n",
    "    # Display run summary\n",
    "    print(f\"\\n📋 Run Summary:\")\n",
    "    print(f\"   Run ID: {current_run.id}\")\n",
    "    print(f\"   Experiment: {current_run.experiment.name}\")\n",
    "    print(f\"   Status: {current_run.get_status()}\")\n",
    "    print(f\"   Portal URL: {current_run.get_portal_url()}\")\n",
    "\n",
    "elif 'current_run' in locals():\n",
    "    print(f\"⏳ Training still in progress. Current status: {current_run.get_status()}\")\n",
    "    print(\"   Run this cell again after training completes\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No training run available for analysis\")\n",
    "    print(\"   Please submit and complete a training job first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment and ADX Integration\n",
    "\n",
    "### Using Trained Models in Azure Data Explorer\n",
    "\n",
    "Azure Data Explorer's Python plugin allows us to run trained models directly within KQL queries. This provides:\n",
    "\n",
    "- **Real-time Inference**: Score data as it arrives in ADX\n",
    "- **Scalability**: Leverage ADX's distributed architecture\n",
    "- **Integration**: Keep ML workflows within your data platform\n",
    "- **Efficiency**: Avoid data movement between systems\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "There are several ways to deploy models for use with ADX:\n",
    "\n",
    "1. **Inline Model**: Embed the model directly in KQL queries (suitable for small models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Model for ADX Deployment\n",
    "\n",
    "We'll create a simple deployment package that includes the model and prediction logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097180047
    }
   },
   "outputs": [],
   "source": [
    "# Missing ADX Model Storage Steps (Original Approach)\n",
    "\n",
    "print(\"📦 STORING MODEL IN ADX TABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: Get the downloaded model file\n",
    "model_files = [f for f in os.listdir('.') if f.endswith('.pkl')]\n",
    "\n",
    "if model_files:\n",
    "    model_path = model_files[0]\n",
    "    print(f\"✅ Using model file: {model_path}\")\n",
    "    \n",
    "    # Step 2: Serialize model to hex string\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    \n",
    "    models_tbl = 'ML_Models'\n",
    "    model_name = 'AML-Occupancy'\n",
    "    \n",
    "    print(f\"\\n📊 Converting model to hex string...\")\n",
    "    \n",
    "    with open(model_path, 'rb') as handle:\n",
    "        buf = handle.read()\n",
    "    \n",
    "    # Convert binary to hex string (key step from original!)\n",
    "    smodel = buf.hex()\n",
    "    \n",
    "    print(f\"   Model size: {len(buf)} bytes\")\n",
    "    print(f\"   Hex string length: {len(smodel)} characters\")\n",
    "    \n",
    "    # Create DataFrame for ADX\n",
    "    now = datetime.datetime.now()\n",
    "    dfm = pd.DataFrame({\n",
    "        'name': [model_name], \n",
    "        'timestamp': [now], \n",
    "        'model': [smodel]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n🏗️ Creating ADX table and storing model...\")\n",
    "    \n",
    "    # Create the ML_Models table\n",
    "    create_table_query = f'''\n",
    ".create table {models_tbl} (\n",
    "    name: string,\n",
    "    timestamp: datetime,\n",
    "    model: string\n",
    ")\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        %kql create_result << -query create_table_query\n",
    "        print(\"✅ ML_Models table created\")\n",
    "    except:\n",
    "        print(\"✅ ML_Models table already exists\")\n",
    "    \n",
    "    # Store the model in the table\n",
    "    ingest_query = f'''\n",
    ".ingest inline into table {models_tbl} <|\n",
    "{model_name},{now.isoformat()},{smodel}\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        %kql ingest_result << -query ingest_query\n",
    "        print(\"✅ Model stored in ADX table!\")\n",
    "        \n",
    "        # Verify storage\n",
    "        verify_query = f'{models_tbl} | project name, timestamp, model_size = strlen(model)'\n",
    "        %kql verify_result << -query verify_query\n",
    "        \n",
    "        if not verify_result.empty:\n",
    "            print(\"✅ Model verification successful!\")\n",
    "            display(verify_result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Storage failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No model file found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ADX Scoring Queries\n",
    "\n",
    "Create ready-to-use KQL queries that incorporate the trained model for real-time predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097194744
    }
   },
   "outputs": [],
   "source": [
    "# ADX Model Scoring Queries (using stored model from table)\n",
    "\n",
    "print(\"🎯 CREATING ADX SCORING QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Use the variable names from the previous step\n",
    "MODEL_TABLE = 'ML_Models'\n",
    "MODEL_NAME = 'AML-Occupancy'\n",
    "\n",
    "if 'MODEL_TABLE' in locals() and 'MODEL_NAME' in locals():\n",
    "    \n",
    "    print(f\"✅ Using stored model:\")\n",
    "    print(f\"   Table: {MODEL_TABLE}\")\n",
    "    print(f\"   Model: {MODEL_NAME}\")\n",
    "    \n",
    "    # Create scoring query that loads model from ADX table\n",
    "    scoring_query = f'''\n",
    "let model_data = toscalar(\n",
    "    {MODEL_TABLE}\n",
    "    | where name == \"{MODEL_NAME}\"\n",
    "    | top 1 by timestamp desc\n",
    "    | project model\n",
    ");\n",
    "OccupancyDetection\n",
    "| take 5\n",
    "| extend prediction = python(typeof(*, prediction:bool, confidence:real),\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load model from hex string stored in ADX\n",
    "model_hex = model_data\n",
    "model_bytes = bytes.fromhex(model_hex)\n",
    "model = pickle.loads(model_bytes)\n",
    "# Make predictions\n",
    "features = ['Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio']\n",
    "X = df[features]\n",
    "predictions = model.predict(X)\n",
    "df['prediction'] = predictions\n",
    "# Add confidence if available\n",
    "if hasattr(model, 'predict_proba'):\n",
    "probabilities = model.predict_proba(X)\n",
    "df['confidence'] = np.max(probabilities, axis=1)\n",
    "else:\n",
    "df['confidence'] = 1.0\n",
    "result = df\n",
    ")\n",
    "| project Timestamp, Temperature, Humidity, Light, CO2, Occupancy, prediction, confidence\n",
    "'''\n",
    "    \n",
    "    print(\"\\n📋 Scoring query created!\")\n",
    "    print(\"   This query loads the model from ADX table and scores new data\")\n",
    "    \n",
    "    # Store for testing\n",
    "    SCORING_QUERY = scoring_query\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Model storage variables not found\")\n",
    "    print(\"   Please run the model storage step first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Testing the Scoring Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1751097359880
    }
   },
   "outputs": [],
   "source": [
    "# Correct ADX Scoring using the Original Approach (Stored Functions)\n",
    "\n",
    "print(\"🎯 USING ORIGINAL ADX SCORING APPROACH\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# The original approach uses 'evaluate python()' with stored functions\n",
    "# This is different from the python() plugin we were trying to use\n",
    "\n",
    "scoring_from_table_query = r'''\n",
    "let classify_sf=(samples:(*), models_tbl:(name:string, timestamp:datetime, model:string), model_name:string, features_cols:dynamic, pred_col:string)\n",
    "{\n",
    "    let model_str = toscalar(models_tbl | where name == model_name | top 1 by timestamp desc | project model);\n",
    "    let kwargs = pack('smodel', model_str, 'features_cols', features_cols, 'pred_col', pred_col);\n",
    "    let code =\n",
    "    'import pickle\\n'\n",
    "    'import binascii\\n'\n",
    "    '\\n'\n",
    "    'smodel = kargs[\"smodel\"]\\n'\n",
    "    'features_cols = kargs[\"features_cols\"]\\n'\n",
    "    'pred_col = kargs[\"pred_col\"]\\n'\n",
    "    'bmodel = binascii.unhexlify(smodel)\\n'\n",
    "    'clf1 = pickle.loads(bmodel)\\n'\n",
    "    'df1 = df[features_cols]\\n'\n",
    "    'predictions = clf1.predict(df1)\\n'\n",
    "    '\\n'\n",
    "    'result = df\\n'\n",
    "    'result[pred_col] = pd.DataFrame(predictions, columns=[pred_col])'\n",
    "    '\\n'\n",
    "    ;\n",
    "    samples | evaluate python(typeof(*), code, kwargs)\n",
    "};\n",
    "OccupancyDetection \n",
    "| where Test == 1\n",
    "| extend pred_Occupancy=bool(0)\n",
    "| invoke classify_sf(ML_Models, 'AML-Occupancy', pack_array('Temperature', 'Humidity', 'Light', 'CO2', 'HumidityRatio'), 'pred_Occupancy')\n",
    "| summarize n=count() by Occupancy, pred_Occupancy\n",
    "'''\n",
    "\n",
    "print(\"📋 Original scoring query recreated!\")\n",
    "print(\"   Uses: evaluate python() instead of python() plugin\")\n",
    "print(\"   Uses: Stored functions with invoke\")\n",
    "print(\"   Uses: binascii.unhexlify() for hex decoding\")\n",
    "\n",
    "print(\"\\n🧪 Testing the original approach...\")\n",
    "\n",
    "scoring_results = None\n",
    "\n",
    "try:\n",
    "    %kql scoring_results << -query scoring_from_table_query\n",
    "    \n",
    "    if scoring_results is not None and not scoring_results.empty:\n",
    "        print(\"✅ SUCCESS! Original approach works!\")\n",
    "        print(f\"   Generated confusion matrix with {len(scoring_results)} rows\")\n",
    "        \n",
    "        print(\"\\n📊 Confusion Matrix Results:\")\n",
    "        display(scoring_results)\n",
    "        \n",
    "        # Calculate accuracy from confusion matrix\n",
    "        if len(scoring_results) > 0:\n",
    "            total_predictions = scoring_results['n'].sum()\n",
    "            correct_predictions = scoring_results[scoring_results['Occupancy'] == scoring_results['pred_Occupancy']]['n'].sum()\n",
    "            accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "            \n",
    "            print(f\"\\n🎯 Model Performance:\")\n",
    "            print(f\"   Total predictions: {total_predictions}\")\n",
    "            print(f\"   Correct predictions: {correct_predictions}\")\n",
    "            print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        print(\"\\n🎉 COMPLETE SUCCESS!\")\n",
    "        print(\"   ✅ Model training: Complete\")\n",
    "        print(\"   ✅ Model storage in ADX: Complete\")\n",
    "        print(\"   ✅ Model scoring in ADX: Complete\")\n",
    "        print(\"   ✅ End-to-end pipeline: Working!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Original approach returned no results\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Original approach failed: {e}\")\n",
    "    \n",
    "    # Let's try a simpler version first\n",
    "    print(\"\\n🔧 Trying simplified version...\")\n",
    "    \n",
    "    simple_scoring_query = r'''\n",
    "let model_str = toscalar(ML_Models | where name == 'AML-Occupancy' | top 1 by timestamp desc | project model);\n",
    "OccupancyDetection \n",
    "| take 3\n",
    "| extend model_available = isnotnull(model_str)\n",
    "| project Timestamp, Temperature, Occupancy, model_available\n",
    "'''\n",
    "    \n",
    "    try:\n",
    "        %kql simple_scoring << -query simple_scoring_query\n",
    "        \n",
    "        if 'simple_scoring' in locals() and not simple_scoring.empty:\n",
    "            print(\"✅ Model retrieval works!\")\n",
    "            display(simple_scoring)\n",
    "        else:\n",
    "            print(\"❌ Model retrieval failed\")\n",
    "            \n",
    "    except Exception as simple_e:\n",
    "        print(f\"❌ Simple test failed: {simple_e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"=\"*60)\n",
    "print(\"The original approach uses:\")\n",
    "print(\"   • 'evaluate python()' - different from 'python()' plugin\")\n",
    "print(\"   • 'invoke' with stored functions\")\n",
    "print(\"   • 'binascii.unhexlify()' instead of 'bytes.fromhex()'\")\n",
    "print(\"\\nThis may be supported even when python() plugin callouts aren't!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "This tutorial demonstrated a complete end-to-end ML workflow using Azure Machine Learning and Azure Data Explorer:\n",
    "\n",
    "✅ **Data Management**: Stored and queried sensor data efficiently in ADX  \n",
    "✅ **Model Training**: Trained multiple classification models using Azure ML compute  \n",
    "✅ **Model Evaluation**: Compared performance across different algorithms  \n",
    "✅ **Model Deployment**: Deployed models for real-time inference in ADX  \n",
    "✅ **Integration**: Seamlessly connected ADX and Azure ML workflows  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Azure Data Explorer** provides powerful capabilities for both data storage and ML inference\n",
    "- **Azure Machine Learning** offers managed, scalable compute for model training\n",
    "- **Python Plugin** in ADX enables sophisticated data processing and ML operations\n",
    "- **Modern SDK patterns** like ScriptRunConfig provide flexibility and maintainability\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [Azure Machine Learning Documentation](https://docs.microsoft.com/azure/machine-learning/)\n",
    "- [Azure Data Explorer Documentation](https://docs.microsoft.com/azure/data-explorer/)\n",
    "- [KQL Reference Guide](https://docs.microsoft.com/azure/data-explorer/kusto/query/)\n",
    "- [Azure ML Python SDK Reference](https://docs.microsoft.com/python/api/overview/azure/ml/)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for following this tutorial!** You now have a solid foundation for building production-ready ML solutions that leverage the power of both Azure Machine Learning and Azure Data Explorer.\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
